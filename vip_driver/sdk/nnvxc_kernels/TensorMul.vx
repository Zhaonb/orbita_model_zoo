#pragma OPENCL EXTENSION cl_viv_vx_extension : enable

#include "cl_viv_vx_ext.h"

_viv_uniform VXC_512Bits uniDataSubZPtoFp32Part0_4x4;
_viv_uniform VXC_512Bits uniDataSubZPtoFp32Part1_4x4;
_viv_uniform VXC_512Bits uniExtact8Bin_2x8;
_viv_uniform int inputZP0;
_viv_uniform int inputZP1;
_viv_uniform float outputZP;

_viv_uniform float input_scale;

#define TENSORMUL(name0, name1, name2, name3, name4, input_type0, copy_type0, input_type1, copy_type1, output_type, convert_type, copy_type, policy) \
    __kernel void vxcTensorEltwise_##name0##Mul##name1##to##name2##_##name3##_##name4##_func( \
    __read_only  image2d_array_t input0, \
    __read_only  image2d_array_t input1, \
    __write_only image2d_array_t output, \
    global float * scale) \
{\
    int4 coord = (int4)(get_global_id(0), get_global_id(1), get_global_id(2), 0);\
    vxc_float4 vecA, vecB, vecC, vecD;\
    input_type0 srcA;\
    copy_type0  src0;\
    input_type1 srcB;\
    copy_type1  src1;\
    input_type0 input_ZP0;\
    input_type1 input_ZP1;\
    VXC_ReadImage2DArray(srcA, input0, coord, VXC_5BITOFFSET_XY(0,0), VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));\
    _viv_asm(COPY, src0, srcA, 16); \
    VXC_ReadImage2DArray(srcB, input1, coord, VXC_5BITOFFSET_XY(0,0), VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));\
    _viv_asm(COPY, src1, srcB, 16); \
    float scaleVal = *scale * input_scale; \
    \
    _viv_asm(COPY, input_ZP0, inputZP0, 4);\
    VXC_DP4x4(vecA, src0, input_ZP0, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part0_4x4);\
    VXC_DP4x4(vecB, src0, input_ZP0, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part1_4x4);\
    _viv_asm(COPY, input_ZP1, inputZP1, 4);\
    VXC_DP4x4(vecC, src1, input_ZP1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part0_4x4);\
    VXC_DP4x4(vecD, src1, input_ZP1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part1_4x4);\
    \
    vecA = vecA * scaleVal;\
    vecB = vecB * scaleVal;\
    vecA = vecA * vecC + outputZP;\
    vecB = vecB * vecD + outputZP;\
    convert_type dst0, dst1;\
    _viv_asm(CONV_##name4, dst0, vecA);\
    _viv_asm(CONV_##name4, dst1, vecB);\
    output_type dst2;\
    VXC_DP2x8(dst2, dst0, dst1, VXC_MODIFIER(0, 7, 0, VXC_RM_ToNearestEven, policy), uniExtact8Bin_2x8);\
    copy_type dst;\
    _viv_asm(COPY, dst, dst2, 16); \
    VXC_WriteImage2DArray(output, coord, dst, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));\
}
//       name0, name1, name2, name3, name4, input_type0, copy_type0,  input_type1, copy_type1,  output_type, convert_type, copy_type,   policy
TENSORMUL(Int8,  Int8,  Int8,  Sat,   RTZ, vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Int8,  Int8,  Fp16,  Sat,   RTZ, vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Int8,  Fp16,  Int8,  Sat,   RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Int8,  Fp16,  Fp16,  Sat,   RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Int8,  Int16, Int8,  Sat,   RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Int8,  Int16, Int16, Sat,   RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Int16, Int16, Int16, Sat,   RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Int16, Int16, Fp16,  Sat,   RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Int16, Fp16,  Int16, Sat,   RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Int16, Fp16,  Fp16,  Sat,   RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(UInt8, UInt8, UInt8, Sat,   RTZ, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(UInt8, UInt8, Fp16,  Sat,   RTZ, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(UInt8, Fp16,  UInt8, Sat,   RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(UInt8, Fp16,  Fp16,  Sat,   RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(UInt8, Int16, UInt8, Sat,   RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(UInt8, Int16, Int16, Sat,   RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Fp16,  Fp16,  Fp16,  Sat,   RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Fp16,  Fp16,  Int8,  Sat,   RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Fp16,  Fp16,  Int16, Sat,   RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Fp16,  Fp16,  UInt8, Sat,   RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(Int8,  Int8,  Int8,  Warp,  RTZ, vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Int8,  Int8,  Fp16,  Warp,  RTZ, vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Int8,  Fp16,  Int8,  Warp,  RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Int8,  Fp16,  Fp16,  Warp,  RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Int8,  Int16, Int8,  Warp,  RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Int8,  Int16, Int16, Warp,  RTZ, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Int16, Int16, Int16, Warp,  RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Int16, Int16, Fp16,  Warp,  RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Int16, Fp16,  Int16, Warp,  RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Int16, Fp16,  Fp16,  Warp,  RTZ, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(UInt8, UInt8, UInt8, Warp,  RTZ, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL(UInt8, Fp16,  UInt8, Warp,  RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL(UInt8, UInt8, Fp16,  Warp,  RTZ, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(UInt8, Fp16,  Fp16,  Warp,  RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(UInt8, Int16, UInt8, Warp,  RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL(UInt8, Int16, Int16, Warp,  RTZ, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Fp16,  Fp16,  Fp16,  Warp,  RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Fp16,  Fp16,  Int8,  Warp,  RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Fp16,  Fp16,  Int16, Warp,  RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Fp16,  Fp16,  UInt8, Warp,  RTZ, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)

TENSORMUL(Int8,  Int8,  Int8,  Sat,   RTE, vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Int8,  Int8,  Fp16,  Sat,   RTE, vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Int8,  Fp16,  Int8,  Sat,   RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Int8,  Fp16,  Fp16,  Sat,   RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Int8,  Int16, Int8,  Sat,   RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Int8,  Int16, Int16, Sat,   RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Int16, Int16, Int16, Sat,   RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Int16, Int16, Fp16,  Sat,   RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Int16, Fp16,  Int16, Sat,   RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Int16, Fp16,  Fp16,  Sat,   RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(UInt8, UInt8, UInt8, Sat,   RTE, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(UInt8, UInt8, Fp16,  Sat,   RTE, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(UInt8, Fp16,  UInt8, Sat,   RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(UInt8, Fp16,  Fp16,  Sat,   RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(UInt8, Int16, UInt8, Sat,   RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(UInt8, Int16, Int16, Sat,   RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Fp16,  Fp16,  Fp16,  Sat,   RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL(Fp16,  Fp16,  Int8,  Sat,   RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL(Fp16,  Fp16,  Int16, Sat,   RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL(Fp16,  Fp16,  UInt8, Sat,   RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL(Int8,  Int8,  Int8,  Warp,  RTE, vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Int8,  Int8,  Fp16,  Warp,  RTE,  vxc_char16, vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Int8,  Fp16,  Int8,  Warp,  RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Int8,  Fp16,  Fp16,  Warp,  RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Int8,  Int16, Int8,  Warp,  RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Int8,  Int16, Int16, Warp,  RTE, vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Int16, Int16, Int16, Warp,  RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Int16, Int16, Fp16,  Warp,  RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Int16, Fp16,  Int16, Warp,  RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Int16, Fp16,  Fp16,  Warp,  RTE, vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(UInt8, UInt8, UInt8, Warp,  RTE, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL(UInt8, Fp16,  UInt8, Warp,  RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL(UInt8, UInt8, Fp16,  Warp,  RTE, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(UInt8, Fp16,  Fp16,  Warp,  RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(UInt8, Int16, UInt8, Warp,  RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL(UInt8, Int16, Int16, Warp,  RTE, vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Fp16,  Fp16,  Fp16,  Warp,  RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL(Fp16,  Fp16,  Int8,  Warp,  RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL(Fp16,  Fp16,  Int16, Warp,  RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL(Fp16,  Fp16,  UInt8, Warp,  RTE, vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)



#define TENSORMUL_Z(name0, name1, name2, name3, name4, input_type0, copy_type0, input_type1, copy_type1, output_type, convert_type, copy_type, policy) \
    __kernel void vxcTensorEltwise_##name0##Mul##name1##to##name2##_##name3##_##name4##_Z_func( \
    __read_only  image2d_array_t input0, \
    __read_only  image2d_array_t input1, \
    __write_only image2d_array_t output, \
    global float * scale) \
{\
    int4 coord = (int4)(get_global_id(0), get_global_id(1), get_global_id(2), 0);\
    vxc_float4 vecA, vecB, vecC, vecD;\
    input_type0 srcA;\
    copy_type0  src0;\
    input_type1 srcB;\
    copy_type1  src1;\
    input_type0 input_ZP0;\
    input_type1 input_ZP1;\
    VXC_ReadImage2DArray(srcA, input0, coord, VXC_5BITOFFSET_XY(0,0), VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));\
    _viv_asm(COPY, src0, srcA, 16); \
    VXC_ReadImage(srcB, input1, coord.xy, VXC_5BITOFFSET_XY(0,0), VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));\
    _viv_asm(COPY, src1, srcB, 16); \
    float scaleVal = *scale * input_scale; \
    \
    _viv_asm(COPY, input_ZP0, inputZP0, 4);\
    VXC_DP4x4(vecA, src0, input_ZP0, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part0_4x4);\
    VXC_DP4x4(vecB, src0, input_ZP0, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part1_4x4);\
    _viv_asm(COPY, input_ZP1, inputZP1, 4);\
    VXC_DP4x4(vecC, src1, input_ZP1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part0_4x4);\
    VXC_DP4x4(vecD, src1, input_ZP1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardInf, 0), uniDataSubZPtoFp32Part1_4x4);\
    \
    vecA = vecA * scaleVal;\
    vecB = vecB * scaleVal;\
    vecA = vecA * vecC + outputZP;\
    vecB = vecB * vecD + outputZP;\
    convert_type dst0, dst1;\
    _viv_asm(CONV_##name4, dst0, vecA);\
    _viv_asm(CONV_##name4, dst1, vecB);\
    output_type dst2;\
    VXC_DP2x8(dst2, dst0, dst1, VXC_MODIFIER(0, 7, 0, VXC_RM_ToNearestEven, policy), uniExtact8Bin_2x8);\
    copy_type dst;\
    _viv_asm(COPY, dst, dst2, 16); \
    VXC_WriteImage2DArray(output, coord, dst, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));\
}
//          name0, name1, name2, name3, name4,input_type0, copy_type0,  input_type1, copy_type1,  output_type, convert_type, copy_type,   policy
TENSORMUL_Z(Int8,  Int8,  Int8,  Sat,   RTZ,    vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Int8,  Int8,  Fp16,  Sat,   RTZ,    vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Int8,  Fp16,  Int8,  Sat,   RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Int8,  Fp16,  Fp16,  Sat,   RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Int8,  Int16, Int8,  Sat,   RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Int8,  Int16, Int16, Sat,   RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Int16, Int16, Int16, Sat,   RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Int16, Int16, Fp16,  Sat,   RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Int16, Fp16,  Int16, Sat,   RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Int16, Fp16,  Fp16,  Sat,   RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(UInt8, UInt8, UInt8, Sat,   RTZ,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(UInt8, UInt8, Fp16,  Sat,   RTZ,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(UInt8, Fp16,  UInt8, Sat,   RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(UInt8, Fp16,  Fp16,  Sat,   RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(UInt8, Int16, UInt8, Sat,   RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(UInt8, Int16, Int16, Sat,   RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Fp16,  Fp16,  Fp16,  Sat,   RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Fp16,  Fp16,  Int8,  Sat,   RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Fp16,  Fp16,  Int16, Sat,   RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Fp16,  Fp16,  UInt8, Sat,   RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(Int8,  Int8,  Int8,  Warp,  RTZ,    vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Int8,  Int8,  Fp16,  Warp,  RTZ,     vxc_char16, vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Int8,  Fp16,  Int8,  Warp,  RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Int8,  Fp16,  Fp16,  Warp,  RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Int8,  Int16, Int8,  Warp,  RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Int8,  Int16, Int16, Warp,  RTZ,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Int16, Int16, Int16, Warp,  RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Int16, Int16, Fp16,  Warp,  RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Int16, Fp16,  Int16, Warp,  RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Int16, Fp16,  Fp16,  Warp,  RTZ,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(UInt8, UInt8, UInt8, Warp,  RTZ,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL_Z(UInt8, Fp16,  UInt8, Warp,  RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL_Z(UInt8, UInt8, Fp16,  Warp,  RTZ,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(UInt8, Fp16,  Fp16,  Warp,  RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(UInt8, Int16, UInt8, Warp,  RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL_Z(UInt8, Int16, Int16, Warp,  RTZ,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Fp16,  Fp16,  Fp16,  Warp,  RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Fp16,  Fp16,  Int8,  Warp,  RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Fp16,  Fp16,  Int16, Warp,  RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Fp16,  Fp16,  UInt8, Warp,  RTZ,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)


TENSORMUL_Z(Int8,  Int8,  Int8,  Sat,   RTE,    vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Int8,  Int8,  Fp16,  Sat,   RTE,    vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Int8,  Fp16,  Int8,  Sat,   RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Int8,  Fp16,  Fp16,  Sat,   RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Int8,  Int16, Int8,  Sat,   RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Int8,  Int16, Int16, Sat,   RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Int16, Int16, Int16, Sat,   RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Int16, Int16, Fp16,  Sat,   RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Int16, Fp16,  Int16, Sat,   RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Int16, Fp16,  Fp16,  Sat,   RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(UInt8, UInt8, UInt8, Sat,   RTE,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(UInt8, UInt8, Fp16,  Sat,   RTE,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(UInt8, Fp16,  UInt8, Sat,   RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(UInt8, Fp16,  Fp16,  Sat,   RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(UInt8, Int16, UInt8, Sat,   RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(UInt8, Int16, Int16, Sat,   RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Fp16,  Fp16,  Fp16,  Sat,   RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  1)
TENSORMUL_Z(Fp16,  Fp16,  Int8,  Sat,   RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  1)
TENSORMUL_Z(Fp16,  Fp16,  Int16, Sat,   RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  1)
TENSORMUL_Z(Fp16,  Fp16,  UInt8, Sat,   RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 1)
TENSORMUL_Z(Int8,  Int8,  Int8,  Warp,  RTE,    vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Int8,  Int8,  Fp16,  Warp,  RTE,     vxc_char16, vxc_char16,  vxc_char16,  vxc_char16,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Int8,  Fp16,  Int8,  Warp,  RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Int8,  Fp16,  Fp16,  Warp,  RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Int8,  Int16, Int8,  Warp,  RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Int8,  Int16, Int16, Warp,  RTE,    vxc_char16,  vxc_char16,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Int16, Int16, Int16, Warp,  RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Int16, Int16, Fp16,  Warp,  RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Int16, Fp16,  Int16, Warp,  RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Int16, Fp16,  Fp16,  Warp,  RTE,    vxc_short8,  vxc_short8,  vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(UInt8, UInt8, UInt8, Warp,  RTE,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL_Z(UInt8, Fp16,  UInt8, Warp,  RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL_Z(UInt8, UInt8, Fp16,  Warp,  RTE,    vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_uchar16, vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(UInt8, Fp16,  Fp16,  Warp,  RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(UInt8, Int16, UInt8, Warp,  RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_uchar16, int4,         vxc_uchar16, 0)
TENSORMUL_Z(UInt8, Int16, Int16, Warp,  RTE,    vxc_uchar16, vxc_uchar16, vxc_short8,  vxc_short8,  vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Fp16,  Fp16,  Fp16,  Warp,  RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_half8,   half4,        vxc_short8,  0)
TENSORMUL_Z(Fp16,  Fp16,  Int8,  Warp,  RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_char16,  int4,         vxc_char16,  0)
TENSORMUL_Z(Fp16,  Fp16,  Int16, Warp,  RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_short8,  int4,         vxc_short8,  0)
TENSORMUL_Z(Fp16,  Fp16,  UInt8, Warp,  RTE,    vxc_short8,  vxc_half8,   vxc_short8,  vxc_half8,   vxc_uchar16, int4,         vxc_uchar16, 0)
_viv_uniform VXC_512Bits uniI8MulI8Lo_2x8;
_viv_uniform VXC_512Bits uniI8MulI8Hi_2x8;
__kernel void vxcTensorEltwise_I8MulI8_2D(
__read_only  image2d_array_t input0,
__read_only  image2d_array_t input1,
__write_only image2d_array_t output,
global float * scale)
{
    int2 coord = (int2)(get_global_id(0), get_global_id(1));

    vxc_char16 src0, src1;
    vxc_char16 dst;
    VXC_ReadImage(src0, input0, coord, VXC_5BITOFFSET_XY(0, 0), VXC_MODIFIER(0, 15, 0, VXC_RM_TowardZero, 0));
    VXC_ReadImage(src1, input1, coord, VXC_5BITOFFSET_XY(0, 0), VXC_MODIFIER(0, 15, 0, VXC_RM_TowardZero, 0));

    VXC_DP2x8(dst, src0, src1, VXC_MODIFIER(0, 7, 0, VXC_RM_ToNearestEven, 1), uniI8MulI8Lo_2x8);
    VXC_DP2x8(dst, src0, src1, VXC_MODIFIER(8, 15, 0, VXC_RM_ToNearestEven, 1), uniI8MulI8Hi_2x8);

    VXC_WriteImage(output, coord, dst, VXC_MODIFIER(0, 15, 0, VXC_RM_TowardZero, 0));
}

_viv_uniform VXC_512Bits uniConvBF16toF32_Part0_2x8;
_viv_uniform VXC_512Bits uniConvBF16toF32_Part1_2x8;
_viv_uniform VXC_512Bits uniPackedBF16_2x8;
__kernel void vxcTensorEltwise_BF16MulBF16ToBF16(
    __read_only image2d_array_t   input0,
    __read_only image2d_array_t   input1,
    __write_only image2d_array_t  output,
global float * scale)
{
    int4 coord = (int4)(get_global_id(0), get_global_id(1), get_global_id(2), 0);
    vxc_ushort8 srcA, src;
    vxc_ushort8 srcB;

    VXC_ReadImage2DArray(srcA, input0, coord, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));
    VXC_ReadImage2DArray(srcB, input1, coord, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));

    float scaleVal = *scale;

    float4 vecA0, vecA1;
    float4 vecB0, vecB1;
    vxc_short8 zero = (vxc_short8)(0, 0, 0, 0, 0, 0, 0, 0);
    VXC_DP2x8(src, srcA, zero, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniConvBF16toF32_Part0_2x8);
    _viv_asm(COPY, vecA0, src, 16);
    VXC_DP2x8(src, srcA, zero, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniConvBF16toF32_Part1_2x8);
    _viv_asm(COPY, vecA1, src, 16);

    VXC_DP2x8(src, srcB, zero, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniConvBF16toF32_Part0_2x8);
    _viv_asm(COPY, vecB0, src, 16);
    VXC_DP2x8(src, srcB, zero, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniConvBF16toF32_Part1_2x8);
    _viv_asm(COPY, vecB1, src, 16);

    vecA0 = vecA0 * vecB0 * scaleVal;
    vecA1 = vecA1 * vecB1 * scaleVal;

    _viv_asm(COPY, srcA, vecA0, 16);
    _viv_asm(COPY, srcB, vecA1, 16);
    VXC_DP2x8(src, srcA, srcB, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniPackedBF16_2x8);

    VXC_WriteImage2DArray(output, coord, src, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0));
}
